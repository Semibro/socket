{
  
    
        "post0": {
            "title": "IAB딥러닝 9월 20일",
            "content": "Import . import torch import numpy as np import matplotlib.pyplot as plt . import os os.environ[&#39;KMP_DUPLICATE_LIB_OK&#39;] = &#39;True&#39; # plt.plot 오류 발생 시 사용 . - numpy&#47928;&#48277; &#52280;&#44256;&#51088;&#47308; . (1) reshape : 2단계 참고 . https://guebin.github.io/IP2022/2022/04/06/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%946%EC%9D%BC.html . (2) concatenate, stack : 4단계 참고 . https://guebin.github.io/IP2022/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html . &#54924;&#44480;&#47784;&#54805; . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . &#54924;&#44480;&#47784;&#54805;&#50640;&#49436; &#45936;&#51060;&#53552; &#49373;&#49457; . torch.manual_seed(43052) ones= torch.ones(100) x,_ = torch.randn(100).sort() X = torch.stack([ones, x]).T # torch.stack([ones,x],axis=1) W = torch.tensor([2.5, 4]) ϵ = torch.randn(100) * 0.5 y = X @ W + ϵ . plt.plot(x, y, &#39;o&#39;) plt.plot(x, 2.5 + 4 * x, &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x24e99b92a30&gt;] . &#54924;&#44480;&#47784;&#54805;&#50640;&#49436; &#54617;&#49845;&#51060;&#46976;? . - 파란점만 주어졌을때, 주황색 점선을 추정하는 것. 좀 더 정확하게 말하면 given data로 $ begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$를 최대한 $ begin{bmatrix} 2.5 4 end{bmatrix}$와 비슷하게 찾는것. . given data : $ big {(x_i,y_i) big }_{i=1}^{n}$ . | parameter: ${ bf W}= begin{bmatrix} w_0 w_1 end{bmatrix}$ . | estimated parameter: ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$ . | . plt.plot(x, y, &#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x24e9a331340&gt;] . - 시도: $( hat{w}_0, hat{w}_1)=(-5,10)$을 선택하여 선을 그려보고 적당한지 판단. . plt.plot(x, y, &#39;o&#39;) plt.plot(x, -5 + 10 * x, &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x24836ad4f70&gt;] . - 벡터표현으로 주황색 점선을 계산 . What = torch.tensor([-5.0, 10.0]) . X.shape . torch.Size([100, 2]) . plt.plot(x, y, &#39;o&#39;) plt.plot(x, X @ What, &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x24e9a38ecd0&gt;] . &#54028;&#46972;&#48120;&#53552;&#47484; &#54617;&#49845;&#54616;&#45716; &#48169;&#48277; . - 컴퓨터의 반복계산을 이용하여 추론 (손실함수 + 경사하강법) . - 전략 : 3단계 전략 . stage 1 : 아무 점선이나 그어본다. . | stage 2 : stage 1에서 그은 점선보다 더 좋은 점선으로 바꾼다. . | stage 3 : stage 1 - 2를 반복한다. . | . Stage 1 : &#51076;&#51032;&#51032; &#49440;&#51012; &#44536;&#50612;&#48376;&#45796;. . What = torch.tensor([-5.0, 10.0], requires_grad = True) What . tensor([-5., 10.], requires_grad=True) . yhat = X @ What . plt.plot(x, y, &#39;o&#39;) plt.plot(x, yhat.data, &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x24e9a3eed30&gt;] . Stage 2 : &#52395;&#48264;&#51704; &#49688;&#51221;. &#52572;&#52488;&#51032; &#51216;&#49440;&#50640; &#45824;&#54620; &#51201;&#45817;&#54620; &#51221;&#46020;&#47484; &#54032;&#45800;&#54616;&#44256; &#45908; &#51201;&#45817;&#54620; &#51216;&#49440;&#51004;&#47196; &#50629;&#45936;&#51060;&#53944;&#47484; &#54620;&#45796;. . - &#39;적당한 정도&#39;를 판단하기 위한 장치: loss function 도입! . $loss= sum_{i=1}^{n}(y_i- hat{y}_i)^2= sum_{i=1}^{n}(y_i-( hat{w}_0+ hat{w}_1x_i))^2$ . $=({ bf y}-{ bf hat{y}})^ top({ bf y}-{ bf hat{y}})=({ bf y}-{ bf X}{ bf hat{W}})^ top({ bf y}-{ bf X}{ bf hat{W}})$ . - loss 함수의 특징 . $y_i approx hat{y}_i$ 일수록 loss값이 작다. | $y_i approx hat{y}_i$ 이 되도록 $( hat{w}_0, hat{w}_1)$을 잘 찍으면 loss값이 작다. | (중요) 주황색 점선이 &#39;적당할 수록&#39; loss값이 작다. | . loss = torch.sum((y - yhat) ** 2) loss . tensor(8587.6875, grad_fn=&lt;SumBackward0&gt;) . - 목표 : loss값을 줄이는 것. . - 경사하강법 / 벡터미분 . - 경사하강법으로 loss를 줄이기 위해서는 $ frac{ partial}{ partial { bf W}}loss(w_0,w_1)$의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. (loss.backward()로 하면된다) . loss.backward() . What, What.grad . (tensor([-5., 10.], requires_grad=True), tensor([-1342.2522, 1188.9305])) . $loss(w_0,w_1)=({ bf y}- hat{ bf y})^ top ({ bf y}- hat{ bf y})=({ bf y}-{ bf XW})^ top ({ bf y}-{ bf XW})$ . | $ frac{ partial}{ partial { bf W} }loss(w_0,w_1)=-2{ bf X}^ top { bf y}+2{ bf X}^ top { bf X W}$ . | . -2 * X.T @ y + 2 * X.T @ X @ What . tensor([-1342.2522, 1188.9308], grad_fn=&lt;AddBackward0&gt;) . $ frac{ partial}{ partial { bf W} } loss(w_0,w_1)= begin{bmatrix} frac{ partial}{ partial w_0} frac{ partial}{ partial w_1} end{bmatrix}loss(w_0,w_1) = begin{bmatrix} frac{ partial}{ partial w_0}loss(w_0,w_1) frac{ partial}{ partial w_1}loss(w_0,w_1) end{bmatrix}$ . | $ frac{ partial}{ partial w_0}loss(w_0,w_1) approx frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}$ . | $ frac{ partial}{ partial w_1}loss(w_0,w_1) approx frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}$ . | . _lossfn = lambda w0, w1 : torch.sum((y - w0 - w1 * x) ** 2) _lossfn(-5, 10) . tensor(8587.6875) . h = 0.001 (_lossfn(-5 + h, 10) - _lossfn(-5, 10)) / h, (_lossfn(-5, 10 + h) - _lossfn(-5, 10)) / h . (tensor(-1341.7968), tensor(1190.4297)) . - 수정 전, 수정하는폭, 수정 후의 값 . alpha=0.001 print(&#39;수정전 : &#39; + str(What.data)) # What 에서 미분꼬리표를 떼고 싶다면? What.data or What.detach() print(&#39;수정하는폭 : &#39; + str(-alpha * What.grad)) print(&#39;수정후 : &#39; + str(What.data - alpha * What.grad)) print(&#39;*참값 : (2.5, 4)&#39; ) . 수정전 : tensor([-5., 10.]) 수정하는폭 : tensor([ 1.3423, -1.1889]) 수정후 : tensor([-3.6577, 8.8111]) *참값 : (2.5, 4) . - Wbefore, Wafter 계산 . Wbefore = What.data Wafter = What.data - alpha * What.grad Wbefore, Wafter . (tensor([-5., 10.]), tensor([-3.6577, 8.8111])) . - Wbefore, Wafter 시각화 . plt.plot(x, y, &#39;o&#39;) plt.plot(x, X @ Wbefore, &#39;--b&#39;) # 파란색 점선 plt.plot(x, X @ Wafter, &#39;--&#39;) # 주황색 점선 . [&lt;matplotlib.lines.Line2D at 0x24ea04d41c0&gt;] . Stage3: Learn (=estimate $ bf hat{W})$ . - 이 과정은 Stage 1, 2를 반복 . What = torch.tensor([-5.0, 10.0], requires_grad = True) . alpha = 0.001 for epoch in range(30): yhat = X @ What loss = torch.sum((y - yhat) ** 2) loss.backward() What.data = What.data - alpha * What.grad What.grad = None . - 반복결과 . What.data . tensor([2.4290, 4.0144]) . - 반복결과 시각화 . plt.plot(x, y, &#39;o&#39;) plt.plot(x, X @ What.data, &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x24ea6bfba60&gt;] . &#54617;&#49845;&#44284;&#51221; &#44592;&#47197; . loss_history = [] yhat_history = [] What_history = [] . What = torch.tensor([-5.0, 10.0], requires_grad = True) alpha = 0.001 for epoch in range(30): yhat = X @ What yhat_history.append(yhat.data.tolist()) loss = torch.sum((y - yhat) ** 2) loss_history.append(loss.item()) loss.backward() What.data = What.data - alpha * What.grad What_history.append(What.data.tolist()) What.grad = None . - epoch = 3 관찰 . plt.plot(x, y, &#39;o&#39;) plt.plot(x, yhat_history[2], &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x24ea6c98ac0&gt;] . - epoch = 10 관찰 . plt.plot(x, y, &#39;o&#39;) plt.plot(x, yhat_history[9], &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x24ea6ce3b80&gt;] . - epoch = 30 관찰 . plt.plot(x, y, &#39;o&#39;) plt.plot(x, yhat_history[29], &#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x24ea6d40eb0&gt;] . - $ hat{ bf W}$ 관찰 . What_history . [[-3.657747745513916, 8.81106948852539], [-2.554811477661133, 7.861191749572754], [-1.649186134338379, 7.101552963256836], [-0.9060712456703186, 6.49347448348999], [-0.2966785430908203, 6.006272315979004], [0.20277434587478638, 5.615575313568115], [0.6119105219841003, 5.302003383636475], [0.9469034671783447, 5.050129413604736], [1.2210699319839478, 4.847657680511475], [1.4453645944595337, 4.684779167175293], [1.6287915706634521, 4.553659439086914], [1.778746247291565, 4.448036193847656], [1.90129816532135, 4.3628973960876465], [2.0014259815216064, 4.294229507446289], [2.0832109451293945, 4.238814353942871], [2.149996757507324, 4.194070339202881], [2.204521894454956, 4.157923698425293], [2.249027729034424, 4.128708839416504], [2.285348415374756, 4.105085849761963], [2.31498384475708, 4.0859761238098145], [2.339160442352295, 4.070511341094971], [2.3588807582855225, 4.057991027832031], [2.3749637603759766, 4.0478515625], [2.3880786895751953, 4.039637088775635], [2.3987717628479004, 4.032979965209961], [2.40748929977417, 4.027583599090576], [2.414595603942871, 4.023208141326904], [2.4203879833221436, 4.019659042358398], [2.4251089096069336, 4.016779899597168], [2.4289560317993164, 4.014443874359131]] . - loss 관찰 . plt.plot(loss_history) . [&lt;matplotlib.lines.Line2D at 0x24ea6d9dc40&gt;] . &#54617;&#49845;&#44284;&#51221;&#51012; animation&#51012; &#51060;&#50857;&#54644; &#49884;&#44033;&#54868; . from matplotlib import animation . plt.rcParams[&#39;figure.figsize&#39;] = (7.5, 2.5) plt.rcParams[&#39;animation.html&#39;] = &#39;jshtml&#39; . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection = &#39;3d&#39;) . - 왼쪽 plot . ax1.plot(x, y, &#39;o&#39;) line, = ax1.plot(x, yhat_history[0]) . fig . - 오른쪽 plot . _w0 = np.arange(-6, 11, 0.5) _w1 = np.arange(-6, 11, 0.5) w1, w0 = np.meshgrid(_w1, _w0) lss = w0 * 0 for i in range(len(_w0)): for j in range(len(_w1)): lss[i, j] = torch.sum((y - _w0[i] - _w1[j] * x) ** 2) ax2.plot_surface(w0, w1, lss, rstride = 1, cstride = 1, color = &#39;b&#39;, alpha = 0.35) ax2.azim = 40 ax2.dist = 8 ax2.elev = 5 . fig . ax2.scatter(2.5, 4, torch.sum((y - 2.5 - 4 * x) ** 2), s = 200, color = &#39;red&#39;, marker = &#39;*&#39;) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x24ea6ed6dc0&gt; . fig . - 오른쪽 plot : $(w_0,w_1)=(-3.66, 8.81)$ 와 $loss(-3.66,8.81)$ 값 . What_history[0] . [-3.657747745513916, 8.81106948852539] . ax2.scatter(What_history[0][0], What_history[0][1], loss_history[0], color = &#39;blue&#39;) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x24ea6f13820&gt; . fig . - 애니메이션 . def animate(epoch): line.set_ydata(yhat_history[epoch]) ax2.scatter(What_history[epoch][0], What_history[epoch][1], loss_history[epoch], color = &#39;blue&#39;) return line ani = animation.FuncAnimation(fig, animate, frames = 30) plt.close() ani . &lt;/input&gt; Once Loop Reflect - 위의 내용을 함수로 합침 . def show_lrpr(data, history): x, y = data loss_history, yhat_history, What_history = history fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection = &#39;3d&#39;) # ax1 --&gt; 왼쪽 plot ax1.plot(x, y, &#39;o&#39;) line, = ax1.plot(x, yhat_history[0]) # ax2 --&gt; 오른쪽 plot _w0 = np.arange(-6, 11, 0.5) _w1 = np.arange(-6, 11, 0.5) w1, w0 = np.meshgrid(_w1, _w0) lss = w0 * 0 for i in range(len(_w0)): for j in range(len(_w1)): lss[i, j] = torch.sum((y - _w0[i] - _w1[j] * x) ** 2) ax2.plot_surface(w0, w1, lss, rstride = 1, cstride = 1, color = &#39;b&#39;, alpha = 0.35) # 3d 곡면 생성 ax2.scatter(2.5, 4, torch.sum((y - 2.5 - 4 * x) ** 2), s = 200, color = &#39;red&#39;, marker = &#39;*&#39;) # 최소점 표시 ax2.scatter(What_history[0][0], What_history[0][1], loss_history[0], color = &#39;b&#39;) ax2.azim = 40 ax2.dist = 8 ax2.elev = 5 def animate(epoch): line.set_ydata(yhat_history[epoch]) ax2.scatter(np.array(What_history)[epoch, 0], np.array(What_history)[epoch, 1], loss_history[epoch], color = &#39;gray&#39;) return line ani = animation.FuncAnimation(fig, animate, frames = 30) plt.close() return ani . show_lrpr([x, y], [loss_history, yhat_history, What_history]) . &lt;/input&gt; Once Loop Reflect $ alpha$&#50640; &#45824;&#54616;&#50668; ($ alpha$&#45716; &#54617;&#49845;&#47456;) . (1) $ alpha$ = 0.0001 : $ alpha$&#44032; &#45320;&#47924; &#51089;&#51004;&#47732; --&gt; &#48708;&#54952;&#50984;&#51201; . loss_history = [] yhat_history = [] What_history = [] . What = torch.tensor([-5.0, 10.0], requires_grad = True) alpha = 0.0001 for epoch in range(30): yhat = X @ What yhat_history.append(yhat.data.tolist()) loss = torch.sum((y - yhat) ** 2) loss_history.append(loss.item()) loss.backward() What.data = What.data - alpha * What.grad What_history.append(What.data.tolist()) What.grad = None . show_lrpr([x, y], [loss_history, yhat_history, What_history]) . &lt;/input&gt; Once Loop Reflect (2) $ alpha$ = 0.0083 : $ alpha$&#44032; &#45320;&#47924; &#53356;&#45796;&#47732; --&gt; &#45796;&#47480; &#51032;&#48120;&#50640;&#49436; &#48708;&#54952;&#50984;&#51201; + &#50948;&#54744; . loss_history = [] yhat_history = [] What_history = [] . What = torch.tensor([-5.0, 10.0], requires_grad = True) alpha = 0.0083 for epoch in range(30): yhat = X @ What yhat_history.append(yhat.data.tolist()) loss = torch.sum((y - yhat) ** 2) loss_history.append(loss.item()) loss.backward() What.data = What.data - alpha * What.grad What_history.append(What.data.tolist()) What.grad = None . show_lrpr([x, y], [loss_history, yhat_history, What_history]) . &lt;/input&gt; Once Loop Reflect (3) $ alpha$ = 0.0085 . loss_history = [] yhat_history = [] What_history = [] . What = torch.tensor([-5.0, 10.0], requires_grad = True) alpha = 0.0085 for epoch in range(30): yhat = X @ What yhat_history.append(yhat.data.tolist()) loss = torch.sum((y - yhat) ** 2) loss_history.append(loss.item()) loss.backward() What.data = What.data - alpha * What.grad What_history.append(What.data.tolist()) What.grad = None . show_lrpr([x, y], [loss_history, yhat_history, What_history]) . &lt;/input&gt; Once Loop Reflect (3) $ alpha$ = 0.01 . loss_history = [] yhat_history = [] What_history = [] . What = torch.tensor([-5.0, 10.0], requires_grad = True) alpha = 0.01 for epoch in range(30): yhat = X @ What yhat_history.append(yhat.data.tolist()) loss = torch.sum((y - yhat) ** 2) loss_history.append(loss.item()) loss.backward() What.data = What.data - alpha * What.grad What_history.append(What.data.tolist()) What.grad = None . show_lrpr([x, y], [loss_history, yhat_history, What_history]) . &lt;/input&gt; Once Loop Reflect - IAB 딥러닝 수업 자료를 기반으로 공부한 내용입니다. .",
            "url": "https://semibro.github.io/socket/2022/09/20/IABDL.html",
            "relUrl": "/2022/09/20/IABDL.html",
            "date": " • Sep 20, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "IAB딥러닝 9월 19일",
            "content": "from fastai.vision.all import * from fastai.collab import * from fastai.text.all import * . 1. &#51060;&#48120;&#51648;&#51088;&#47308;&#48516;&#49437; . path = untar_data(URLs.MNIST_SAMPLE) . dls = ImageDataLoaders.from_folder(path, suffle = False) . Due to IPython and Windows limitation, python multiprocessing isn&#39;t available now. So `number_workers` is changed to 0 to avoid getting stuck . dls.show_batch() . - cnn_learner를 이용하여 lrnr 오브젝트 생성 . arch = resnet34 | metrics = error_rate | . lrnr = vision_learner(dls, arch = resnet34, metrics = error_rate) . lrnr.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.290875 | 0.139522 | 0.046614 | 00:23 | . epoch train_loss valid_loss error_rate time . 0 | 0.053940 | 0.020497 | 0.006869 | 00:13 | . X, y = dls.one_batch() . X.shape # 64개의 이미지, 3채널, (28, 28)크기 . torch.Size([64, 3, 28, 28]) . show_image(X[0]) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . &lt;AxesSubplot:&gt; . lrnr.model(X[0].reshape(1, 3, 28, 28)) . TensorBase([[-4.9088, 5.3898]], device=&#39;cuda:0&#39;, grad_fn=&lt;AliasBackward0&gt;) . import numpy as np a = np.exp(-4.9088) b = np.exp(5.3898) print(&#39;3일 확률 : &#39;, a / (a + b)) print(&#39;7일 확률 : &#39;, b / (a + b)) . 3일 확률 : 3.3679080176154866e-05 7일 확률 : 0.9999663209198238 . lrnr.predict(X[0].to(&quot;cpu&quot;)) . (&#39;3&#39;, TensorBase(0), TensorBase([0.9966, 0.0034])) . 2. &#52628;&#52380;&#49884;&#49828;&#53596; . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv&#39;) df . user item rating item_name . 0 1 | 15 | 1.084308 | 홍차5 | . 1 1 | 1 | 4.149209 | 커피1 | . 2 1 | 11 | 1.142659 | 홍차1 | . 3 1 | 5 | 4.033415 | 커피5 | . 4 1 | 4 | 4.078139 | 커피4 | . ... ... | ... | ... | ... | . 995 100 | 18 | 4.104276 | 홍차8 | . 996 100 | 17 | 4.164773 | 홍차7 | . 997 100 | 14 | 4.026915 | 홍차4 | . 998 100 | 4 | 0.838720 | 커피4 | . 999 100 | 7 | 1.094826 | 커피7 | . 1000 rows × 4 columns . (1) 73&#48264; &#50976;&#51200;&#44032; &#47673;&#51008; &#50500;&#51060;&#53596; &#48143; &#54217;&#51216;&#51012; &#52636;&#47141;&#54616;&#45716; &#53076;&#46300;&#47484; &#51089;&#49457;. . df.query(&#39;user == 73&#39;) . user item rating item_name . 720 73 | 20 | 3.733853 | 홍차10 | . 721 73 | 18 | 3.975004 | 홍차8 | . 722 73 | 9 | 1.119541 | 커피9 | . 723 73 | 13 | 3.840801 | 홍차3 | . 724 73 | 2 | 0.943742 | 커피2 | . 725 73 | 4 | 1.152405 | 커피4 | . 726 73 | 1 | 0.887292 | 커피1 | . 727 73 | 7 | 0.947641 | 커피7 | . 728 73 | 6 | 0.868370 | 커피6 | . 729 73 | 17 | 3.873590 | 홍차7 | . (2) dls&#50752; lrnr &#50724;&#48652;&#51229;&#53944;&#47484; &#49373;&#49457;&#54616;&#44256; lrnr &#50724;&#48652;&#51229;&#53944;&#47484; &#54617;&#49845;. . dls = CollabDataLoaders.from_df(df) lrnr = collab_learner(dls, y_range = (0, 5)) . lrnr.fit(50) . epoch train_loss valid_loss time . 0 | 2.308994 | 2.355548 | 00:00 | . 1 | 2.306565 | 2.353840 | 00:00 | . 2 | 2.299696 | 2.340409 | 00:00 | . 3 | 2.280939 | 2.298518 | 00:00 | . 4 | 2.246671 | 2.214348 | 00:00 | . 5 | 2.189545 | 2.079250 | 00:00 | . 6 | 2.106238 | 1.893685 | 00:00 | . 7 | 1.993163 | 1.668546 | 00:00 | . 8 | 1.851504 | 1.418325 | 00:00 | . 9 | 1.686607 | 1.162084 | 00:00 | . 10 | 1.503044 | 0.919690 | 00:00 | . 11 | 1.314056 | 0.702645 | 00:00 | . 12 | 1.128208 | 0.522699 | 00:00 | . 13 | 0.952406 | 0.380159 | 00:00 | . 14 | 0.792922 | 0.274280 | 00:00 | . 15 | 0.653734 | 0.198446 | 00:00 | . 16 | 0.534922 | 0.146163 | 00:00 | . 17 | 0.435490 | 0.111528 | 00:00 | . 18 | 0.353936 | 0.088991 | 00:00 | . 19 | 0.287826 | 0.074637 | 00:00 | . 20 | 0.234859 | 0.064953 | 00:00 | . 21 | 0.192636 | 0.058722 | 00:00 | . 22 | 0.159204 | 0.054913 | 00:00 | . 23 | 0.132806 | 0.051880 | 00:00 | . 24 | 0.111807 | 0.050033 | 00:00 | . 25 | 0.095299 | 0.048970 | 00:00 | . 26 | 0.082377 | 0.047987 | 00:00 | . 27 | 0.071978 | 0.047371 | 00:00 | . 28 | 0.063799 | 0.047291 | 00:00 | . 29 | 0.057256 | 0.047057 | 00:00 | . 30 | 0.052145 | 0.046960 | 00:00 | . 31 | 0.048205 | 0.047008 | 00:00 | . 32 | 0.044957 | 0.046687 | 00:00 | . 33 | 0.042382 | 0.046597 | 00:00 | . 34 | 0.040269 | 0.046746 | 00:00 | . 35 | 0.038481 | 0.046838 | 00:00 | . 36 | 0.037097 | 0.047285 | 00:00 | . 37 | 0.036044 | 0.047355 | 00:00 | . 38 | 0.035167 | 0.047152 | 00:00 | . 39 | 0.034341 | 0.047297 | 00:00 | . 40 | 0.033596 | 0.047628 | 00:00 | . 41 | 0.032875 | 0.047863 | 00:00 | . 42 | 0.032257 | 0.047251 | 00:00 | . 43 | 0.031906 | 0.047225 | 00:00 | . 44 | 0.031499 | 0.047751 | 00:00 | . 45 | 0.030912 | 0.047885 | 00:00 | . 46 | 0.030393 | 0.048134 | 00:00 | . 47 | 0.030251 | 0.048073 | 00:00 | . 48 | 0.029833 | 0.047878 | 00:00 | . 49 | 0.029378 | 0.047627 | 00:00 | . (3) &#45936;&#51060;&#53552;&#54532;&#47112;&#51076; &#49373;&#49457; . df_new = pd.DataFrame({&#39;user&#39;:[73] * 20, &#39;item&#39;:range(1,21)}) df_new . user item . 0 73 | 1 | . 1 73 | 2 | . 2 73 | 3 | . 3 73 | 4 | . 4 73 | 5 | . 5 73 | 6 | . 6 73 | 7 | . 7 73 | 8 | . 8 73 | 9 | . 9 73 | 10 | . 10 73 | 11 | . 11 73 | 12 | . 12 73 | 13 | . 13 73 | 14 | . 14 73 | 15 | . 15 73 | 16 | . 16 73 | 17 | . 17 73 | 18 | . 18 73 | 19 | . 19 73 | 20 | . (4) 73&#48264; &#50976;&#51200;&#51032; &#52712;&#54693; &#54028;&#50501; . _dl = dls.test_dl(df_new) lrnr.get_preds(dl = _dl) . (tensor([0.9909, 1.0178, 1.0145, 1.0174, 0.9919, 0.9393, 1.0213, 1.0092, 1.0326, 0.9973, 3.8045, 3.8595, 3.8676, 3.8691, 3.8351, 3.8488, 3.8202, 3.8834, 3.7705, 3.8070]), None) . 3. &#49884;&#53248;&#49828;&#51088;&#47308;&#48516;&#49437; . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-19-human_numbers_100.csv&#39;) df . Unnamed: 0 text . 0 0 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 1 1 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 2 2 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 3 3 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 4 4 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . ... ... | ... | . 1995 1995 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 1996 1996 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 1997 1997 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 1998 1998 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 1999 1999 | one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve... | . 2000 rows × 2 columns . (1) TextDataLoaders.from_df&#51012; &#51060;&#50857;&#54616;&#50668; dls &#50724;&#48652;&#51229;&#53944; &#49373;&#49457; . dls = TextDataLoaders.from_df(df, is_lm = True, seq_len = 5, text_col = &#39;text&#39;) dls.show_batch() . Due to IPython and Windows limitation, python multiprocessing isn&#39;t available now. So `n_workers` has to be changed to 0 to avoid getting stuck . text text_ . 0 xxbos one , two , | one , two , three | . 1 hundred xxbos one , two | xxbos one , two , | . 2 one hundred xxbos one , | hundred xxbos one , two | . 3 , one hundred xxbos one | one hundred xxbos one , | . 4 nine , one hundred xxbos | , one hundred xxbos one | . 5 ninety nine , one hundred | nine , one hundred xxbos | . 6 , ninety nine , one | ninety nine , one hundred | . 7 eight , ninety nine , | , ninety nine , one | . 8 ninety eight , ninety nine | eight , ninety nine , | . (2) lrnr &#50724;&#48652;&#51229;&#53944; &#49373;&#49457; (arch = AWD_LSTM, metrics = accuracy) . lrnr = language_model_learner(dls, arch = AWD_LSTM, metrics = accuracy) . (3) lrnr &#50724;&#48652;&#51229;&#53944;&#50640;&#49436; fine_tune(3) &#47700;&#49548;&#46300;&#47484; &#51060;&#50857;&#54616;&#50668; &#47784;&#54805;&#51012; &#54617;&#49845; . lrnr.fine_tune(3) . epoch train_loss valid_loss accuracy time . 0 | 0.541494 | 0.157192 | 0.977558 | 00:52 | . epoch train_loss valid_loss accuracy time . 0 | 0.026514 | 0.004046 | 0.999242 | 00:55 | . 1 | 0.002075 | 0.002533 | 0.999315 | 00:55 | . 2 | 0.001414 | 0.002281 | 0.999324 | 00:55 | . lrnr.predict(&#39;one, two,&#39;, n_words = 50) . &#39;one , two , three , four , five , six , seven , eight , nine , ten , eleven , twelve , thirteen , fourteen , fifteen , sixteen , seventeen , eighteen , nineteen , twenty , twenty one , twenty two , twenty three , twenty four , twenty five&#39; . lrnr.predict(&#39;twenty, twenty one,&#39;, n_words = 50) . &#39;twenty , twenty one , thirty two , thirty three , thirty four , thirty five , thirty six , thirty seven , thirty eight , thirty nine , forty , forty one , forty two , forty three , forty four , forty five , forty six , forty seven , forty eight ,&#39; . - IAB 딥러닝 수업 자료를 기반으로 공부한 내용입니다. .",
            "url": "https://semibro.github.io/socket/2022/09/19/IABDL.html",
            "relUrl": "/2022/09/19/IABDL.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "IAB딥러닝 9월 15일",
            "content": "Import . from fastai.vision.all import * from fastai.vision.gan import * . &#51060;&#48120;&#51648;&#48516;&#49437;, &#52628;&#52380;&#49884;&#49828;&#53596;, &#53581;&#49828;&#53944;&#48516;&#49437;&#51032; &#51221;&#47532; . - 데이터는 모두 아래와 비슷하다. . (1) 데이터는 (X, y)의 형태로 정리되어 있음 . (2) y는 알고자 하는 값 즉, y를 적절하게 추정하는 것이 중요 . (3) X는 y를 추정하기 위해 필요한 정보 . $X$ = 설명변수 = 독립변수 $y$ = 반응변수 = 종속변수 비고 순서 예시 . 이미지 | 카테고리 | 합성곱신경망 | 상관없음 | 개/고양이 이미지 구분 | . 유저,아이템 | 평점 | 추천시스템 | 상관없음 | 넷플릭스 영화추천 | . 과거~오늘까지의주가 | 내일주가 | 순환신경망 | 순서상관있음 | 주가예측 | . 처음 $m$개의 단어(혹은 문장) | 이후 1개의 단어(혹은 문장) | 순환신경망 | 순서상관있음 | 챗봇, 텍스트생성 | . 처음 $m$개의 단어(혹은 문장) | 카테고리 | 순환신경망 | 순서상관있음 | 영화리뷰 텍스트 감정분류 | . - 학습이란 주어진 자료 (X, y)를 잘 분석하여 X에서 y로 가는 어떠한 규칙(맵핑, 함수, 모델, 네트워크) 혹은 원리를 찾는 것 . GAN . - GAN은 생성모형 중 하나 . - GAN의 원리는 경찰과 위조지폐범이 서로 경쟁을 통해 발전하는 모형으로 설명 가능 . - 서로 적대적인(adversarial) 네트워크(network)를 동시에 학습시켜 가짜이미지를 생성(generate) . - 상황극 . 위조범 : 가짜 돈을 만들어보자! (가짜 돈 생성) . | 경찰 : 가짜 돈을 판별함 . | 위조범 : 더 정교하게 만들어야겠다! . | 경찰 : 진짜 돈인가? 가짜 돈인가? (구분이 어려워짐) . | 위조범 : 더 더 정교하게 만들자! . | 경찰 : 판별능력을 업그레이드 하자! . | 반복... . | . - 경찰이 진짜 돈과 가짜 돈을 구분하지 못할 때( = 진짜 이미지를 0.5의 확률로 진짜라고 판단할 때 = 가짜 이미지를 0.5의 확률로 가짜라고 판단할 때) 학습을 멈춤 . 1&#45800;&#44228; . path = untar_data(URLs.MNIST_SAMPLE) . . 100.14% [3219456/3214948 00:01&lt;00:00] dblock = DataBlock(blocks = (TransformBlock, ImageBlock), get_x = generate_noise, get_items = get_image_files, item_tfms = Resize(32)) dls = dblock.dataloaders(path) . Due to IPython and Windows limitation, python multiprocessing isn&#39;t available now. So `number_workers` is changed to 0 to avoid getting stuck . dls.show_batch() . 2&#45800;&#44228; . counterfeiter = basic_generator(32, n_channels = 3, n_extra_layers = 1) # 32 * 32 컬러 이미지 출력 police = basic_critic(32, n_channels = 3, n_extra_layers = 1) # 32 * 32 컬러 이미지 입력 . lrnr = GANLearner.wgan(dls,counterfeiter,police) . 3&#45800;&#44228; . lrnr.fit(10) . C: Users USER anaconda3 lib site-packages fastai callback core.py:69: UserWarning: You are shadowing an attribute (generator) that exists in the learner. Use `self.learn.generator` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) C: Users USER anaconda3 lib site-packages fastai callback core.py:69: UserWarning: You are shadowing an attribute (critic) that exists in the learner. Use `self.learn.critic` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) C: Users USER anaconda3 lib site-packages fastai callback core.py:69: UserWarning: You are shadowing an attribute (gen_mode) that exists in the learner. Use `self.learn.gen_mode` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss gen_loss crit_loss time . 0 | -0.536390 | 0.351954 | 0.351954 | -0.755480 | 00:19 | . 1 | -0.575676 | 0.368697 | 0.368697 | -0.764731 | 00:17 | . 2 | -0.579221 | 0.401217 | 0.401217 | -0.761131 | 00:17 | . 3 | -0.578511 | 0.267153 | 0.267153 | -0.764614 | 00:16 | . 4 | -0.577176 | 0.245672 | 0.245672 | -0.754542 | 00:17 | . 5 | -0.561378 | 0.314552 | 0.314552 | -0.735245 | 00:17 | . 6 | -0.563071 | 0.291162 | 0.291162 | -0.738663 | 00:17 | . 7 | -0.551359 | 0.266653 | 0.266653 | -0.733204 | 00:17 | . 8 | -0.535449 | 0.339633 | 0.339633 | -0.735344 | 00:17 | . 9 | -0.502449 | 0.206701 | 0.206701 | -0.687934 | 00:17 | . lrnr.show_results() . lrnr.fit(20) # 추가로 20회 더 진행 . epoch train_loss valid_loss gen_loss crit_loss time . 0 | -0.463692 | 0.356721 | 0.356721 | -0.649763 | 00:17 | . 1 | -0.517553 | 0.192659 | 0.192659 | -0.432233 | 00:17 | . 2 | -0.537279 | 0.309562 | 0.309562 | -0.716175 | 00:17 | . 3 | -0.498665 | 0.248254 | 0.248254 | -0.710489 | 00:17 | . 4 | -0.516934 | 0.245185 | 0.245185 | -0.664505 | 00:17 | . 5 | -0.530214 | 0.292247 | 0.292247 | -0.728173 | 00:16 | . 6 | -0.512573 | 0.280468 | 0.280468 | -0.712356 | 00:16 | . 7 | -0.492679 | 0.284331 | 0.284331 | -0.664606 | 00:16 | . 8 | -0.365144 | 0.193021 | 0.193021 | -0.294480 | 00:16 | . 9 | -0.501201 | 0.332451 | 0.332451 | -0.693011 | 00:16 | . 10 | -0.342812 | 0.284920 | 0.284920 | -0.495047 | 00:16 | . 11 | -0.378736 | -0.022325 | -0.022325 | -0.185245 | 00:16 | . 12 | -0.470577 | 0.248631 | 0.248631 | -0.715788 | 00:16 | . 13 | -0.441847 | 0.215741 | 0.215741 | -0.645985 | 00:16 | . 14 | -0.449947 | -0.244867 | -0.244867 | -0.248203 | 00:16 | . 15 | -0.293098 | 0.146590 | 0.146590 | -0.276759 | 00:16 | . 16 | -0.443047 | 0.297963 | 0.297963 | -0.643778 | 00:16 | . 17 | -0.385012 | 0.259445 | 0.259445 | -0.578367 | 00:16 | . 18 | -0.372060 | 0.210893 | 0.210893 | -0.552931 | 00:16 | . 19 | -0.358579 | 0.086571 | 0.086571 | -0.561554 | 00:16 | . lrnr.show_results() . - IAB 딥러닝 수업 자료를 기반으로 공부한 내용입니다. .",
            "url": "https://semibro.github.io/socket/2022/09/15/IABDL.html",
            "relUrl": "/2022/09/15/IABDL.html",
            "date": " • Sep 15, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "IAB딥러닝 9월 13일",
            "content": "Import . from fastai.collab import * # 추천시스템 from fastai.text.all import * # 텍스트분석 . import pandas as pd . fastai&#47484; &#51060;&#50857;&#54620; &#48516;&#49437; &#45800;&#44228; . 이미지분석(CNN) 추천시스템 텍스트분석 GAN . 1단계 | ImageDataLoaders | CollabDataLoaders | TextDataLoaders | DataBlock -&gt; dls | . 2단계 | cnn_learner() | collab_learner() | language_model_learner() | GANLearner.wgan() | . 3단계 | lrnr.fine_tune(1) | lrnr.fit() | lrnr.fit() | lrnr.fit() | . 4단계 | lrnr.predict(), lrnr.model(X) | lrnr.model(X) | lrnr.predict() | | . &#52628;&#52380;&#49884;&#49828;&#53596; . 1&#45800;&#44228; . df_view = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_view.csv&#39;) df_view . 커피1 커피2 커피3 커피4 커피5 커피6 커피7 커피8 커피9 커피10 홍차1 홍차2 홍차3 홍차4 홍차5 홍차6 홍차7 홍차8 홍차9 홍차10 . 0 4.149209 | NaN | NaN | 4.078139 | 4.033415 | 4.071871 | NaN | NaN | NaN | NaN | 1.142659 | 1.109452 | NaN | 0.603118 | 1.084308 | NaN | 0.906524 | NaN | NaN | 0.903826 | . 1 4.031811 | NaN | NaN | 3.822704 | NaN | NaN | NaN | 4.071410 | 3.996206 | NaN | NaN | 0.839565 | 1.011315 | NaN | 1.120552 | 0.911340 | NaN | 0.860954 | 0.871482 | NaN | . 2 4.082178 | 4.196436 | NaN | 3.956876 | NaN | NaN | NaN | 4.450931 | 3.972090 | NaN | NaN | NaN | NaN | 0.983838 | NaN | 0.918576 | 1.206796 | 0.913116 | NaN | 0.956194 | . 3 NaN | 4.000621 | 3.895570 | NaN | 3.838781 | 3.967183 | NaN | NaN | NaN | 4.105741 | 1.147554 | NaN | 1.346860 | NaN | 0.614099 | 1.297301 | NaN | NaN | NaN | 1.147545 | . 4 NaN | NaN | NaN | NaN | 3.888208 | NaN | 3.970330 | 3.979490 | NaN | 4.010982 | NaN | 0.920995 | 1.081111 | 0.999345 | NaN | 1.195183 | NaN | 0.818332 | 1.236331 | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 95 0.511905 | 1.066144 | NaN | 1.315430 | NaN | 1.285778 | NaN | 0.678400 | 1.023020 | 0.886803 | NaN | 4.055996 | NaN | NaN | 4.156489 | 4.127622 | NaN | NaN | NaN | NaN | . 96 NaN | 1.035022 | NaN | 1.085834 | NaN | 0.812558 | NaN | 1.074543 | NaN | 0.852806 | 3.894772 | NaN | 4.071385 | 3.935935 | NaN | NaN | 3.989815 | NaN | NaN | 4.267142 | . 97 NaN | 1.115511 | NaN | 1.101395 | 0.878614 | NaN | NaN | NaN | 1.329319 | NaN | 4.125190 | NaN | 4.354638 | 3.811209 | 4.144648 | NaN | NaN | 4.116915 | 3.887823 | NaN | . 98 NaN | 0.850794 | NaN | NaN | 0.927884 | 0.669895 | NaN | NaN | 0.665429 | 1.387329 | NaN | NaN | 4.329404 | 4.111706 | 3.960197 | NaN | NaN | NaN | 3.725288 | 4.122072 | . 99 NaN | NaN | 1.413968 | 0.838720 | NaN | NaN | 1.094826 | 0.987888 | NaN | 1.177387 | 3.957383 | 4.136731 | NaN | 4.026915 | NaN | NaN | 4.164773 | 4.104276 | NaN | NaN | . 100 rows × 20 columns . df = pd.read_csv(&#39;https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv&#39;) df . user item rating item_name . 0 1 | 15 | 1.084308 | 홍차5 | . 1 1 | 1 | 4.149209 | 커피1 | . 2 1 | 11 | 1.142659 | 홍차1 | . 3 1 | 5 | 4.033415 | 커피5 | . 4 1 | 4 | 4.078139 | 커피4 | . ... ... | ... | ... | ... | . 995 100 | 18 | 4.104276 | 홍차8 | . 996 100 | 17 | 4.164773 | 홍차7 | . 997 100 | 14 | 4.026915 | 홍차4 | . 998 100 | 4 | 0.838720 | 커피4 | . 999 100 | 7 | 1.094826 | 커피7 | . 1000 rows × 4 columns . df.item.unique(), df.user.unique() # unique()는 데이터에 고유값들이 어떠한 종류들이 있는지 알고 싶을 때 사용하는 함수 . (array([15, 1, 11, 5, 4, 14, 6, 20, 12, 17, 8, 9, 13, 19, 18, 16, 2, 3, 10, 7], dtype=int64), array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100], dtype=int64)) . dls = CollabDataLoaders.from_df(df) #협업 필터링에 적합한 항목 생성 . dls.show_batch() . user item rating . 0 78 | 13 | 4.020114 | . 1 14 | 1 | 4.329083 | . 2 52 | 16 | 4.008471 | . 3 79 | 15 | 4.105639 | . 4 16 | 14 | 0.946549 | . 5 98 | 11 | 4.125190 | . 6 12 | 9 | 4.502565 | . 7 80 | 13 | 3.725410 | . 8 5 | 10 | 4.010983 | . 9 76 | 2 | 0.725603 | . X, y = dls.one_batch() . X[0], y[0] # 60번 유저가 3번 아이템을 먹었을 때, 평점 1.1309 . (tensor([60, 3]), tensor([1.1309])) . 2&#45800;&#44228; . lrnr = collab_learner(dls, y_range = (0, 5)) . lrnr.fit(30) . epoch train_loss valid_loss time . 0 | 0.982962 | 0.960218 | 00:00 | . 1 | 0.854534 | 0.739310 | 00:00 | . 2 | 0.728671 | 0.549827 | 00:00 | . 3 | 0.612306 | 0.398833 | 00:00 | . 4 | 0.508047 | 0.283823 | 00:00 | . 5 | 0.416964 | 0.201994 | 00:00 | . 6 | 0.340235 | 0.146098 | 00:00 | . 7 | 0.277236 | 0.109585 | 00:00 | . 8 | 0.226001 | 0.086219 | 00:00 | . 9 | 0.185047 | 0.071640 | 00:00 | . 10 | 0.152903 | 0.062686 | 00:00 | . 11 | 0.127411 | 0.057081 | 00:00 | . 12 | 0.107212 | 0.053650 | 00:00 | . 13 | 0.091557 | 0.051878 | 00:00 | . 14 | 0.078987 | 0.050590 | 00:00 | . 15 | 0.069260 | 0.050028 | 00:00 | . 16 | 0.061507 | 0.049620 | 00:00 | . 17 | 0.055520 | 0.049438 | 00:00 | . 18 | 0.050674 | 0.049544 | 00:00 | . 19 | 0.046964 | 0.049867 | 00:00 | . 20 | 0.043926 | 0.049945 | 00:00 | . 21 | 0.041574 | 0.050093 | 00:00 | . 22 | 0.039657 | 0.049922 | 00:00 | . 23 | 0.038015 | 0.049928 | 00:00 | . 24 | 0.036744 | 0.049870 | 00:00 | . 25 | 0.035572 | 0.049686 | 00:00 | . 26 | 0.034816 | 0.049891 | 00:00 | . 27 | 0.034127 | 0.049548 | 00:00 | . 28 | 0.033472 | 0.049744 | 00:00 | . 29 | 0.032938 | 0.049897 | 00:00 | . 4&#45800;&#44228; . yhat = lrnr.model(X.to(&quot;cuda:0&quot;)) yhat . tensor([1.0497, 1.0491, 3.8868, 1.0842, 1.0308, 3.9974, 3.9883, 3.9073, 3.8864, 3.9928, 1.0348, 4.0798, 1.0885, 0.9187, 4.1383, 4.0982, 4.1652, 3.8058, 0.9393, 4.0877, 4.0433, 1.0097, 0.8921, 0.8991, 4.1124, 0.9828, 1.0059, 1.0155, 0.9488, 0.9874, 3.9683, 1.0021, 1.0736, 0.9726, 0.9243, 0.9903, 4.0987, 3.9215, 4.0557, 0.8602, 3.8443, 4.0904, 4.0177, 0.9665, 1.0474, 4.0748, 1.1496, 0.9478, 4.0791, 1.1091, 4.0488, 1.0230, 4.0360, 4.0741, 0.7673, 3.9696, 1.0462, 3.9625, 4.1699, 1.0259, 4.0511, 3.9820, 0.9829, 3.9767], device=&#39;cuda:0&#39;, grad_fn=&lt;AddBackward0&gt;) . X.shape . torch.Size([64, 2]) . X[0:1] . tensor([[60, 3]]) . lrnr.model(X[0:1].to(&quot;cuda:0&quot;)) . tensor([1.0497], device=&#39;cuda:0&#39;, grad_fn=&lt;AddBackward0&gt;) . - 새로운 데이터 생성 후 예측 . Xnew = torch.tensor([[1, 2]]) . lrnr.model(Xnew.to(&quot;cuda:0&quot;)) . tensor([3.9995], device=&#39;cuda:0&#39;, grad_fn=&lt;AddBackward0&gt;) . &#53581;&#49828;&#53944;&#48516;&#49437; . 1&#45800;&#44228; . df = pd.DataFrame({&#39;text&#39; : [&#39;h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??&#39;] * 20000}) df . text . 0 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 1 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 2 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 3 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 4 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . ... ... | . 19995 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 19996 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 19997 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 19998 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 19999 h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ?? | . 20000 rows × 1 columns . dls = TextDataLoaders.from_df(df, text_col = &#39;text&#39;, is_lm = True) . Due to IPython and Windows limitation, python multiprocessing isn&#39;t available now. So `n_workers` has to be changed to 0 to avoid getting stuck . dls.show_batch() . text text_ . 0 xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o | h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . | . 1 ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l | xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o | . 2 ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l | ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l | . 3 o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e | ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l | . 4 l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h | o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e | . 5 l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos | l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h | . 6 e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? | l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos | . 7 h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? | e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? | . 8 ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o | h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? | . 2&#45800;&#44228; . lrnr = language_model_learner(dls, AWD_LSTM) . . 100.00% [105070592/105067061 00:07&lt;00:00] lrnr.fit(20) . epoch train_loss valid_loss time . 0 | 0.953486 | 0.372985 | 00:13 | . 1 | 0.617590 | 0.249084 | 00:13 | . 2 | 0.475271 | 0.214262 | 00:13 | . 3 | 0.382338 | 0.186492 | 00:13 | . 4 | 0.365782 | 0.170414 | 00:13 | . 5 | 0.324649 | 0.150192 | 00:13 | . 6 | 0.322575 | 0.135068 | 00:13 | . 7 | 0.297149 | 0.119661 | 00:13 | . 8 | 0.268460 | 0.103948 | 00:13 | . 9 | 0.236526 | 0.085444 | 00:13 | . 10 | 0.234326 | 0.071486 | 00:13 | . 11 | 0.202645 | 0.055195 | 00:13 | . 12 | 0.180042 | 0.041449 | 00:13 | . 13 | 0.181329 | 0.032187 | 00:13 | . 14 | 0.160750 | 0.025342 | 00:13 | . 15 | 0.157359 | 0.021769 | 00:13 | . 16 | 0.141533 | 0.017457 | 00:13 | . 17 | 0.133610 | 0.015023 | 00:13 | . 18 | 0.128645 | 0.012770 | 00:13 | . 19 | 0.100870 | 0.011586 | 00:13 | . lrnr.predict(&#39;h &#39;, n_words = 30) . &#39;h e l l o . h e l l o ! h e l l o ? ? h e l l o . h e l l o&#39; . - IAB 딥러닝 수업 자료를 기반으로 공부한 내용입니다. .",
            "url": "https://semibro.github.io/socket/2022/09/13/IABDL.html",
            "relUrl": "/2022/09/13/IABDL.html",
            "date": " • Sep 13, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "IAB딥러닝 9월 8일",
            "content": "Import . from fastai.vision.all import * . &#48373;&#49845; . (1) &#45936;&#51060;&#53552; &#51221;&#47532; . path = untar_data(URLs.PETS) / &#39;images&#39; . fnames = get_image_files(path) . def label_func(fname): if fname[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . dls = ImageDataLoaders.from_name_func(path, fnames, label_func, item_tfms = Resize(224)) . Due to IPython and Windows limitation, python multiprocessing isn&#39;t available now. So `number_workers` is changed to 0 to avoid getting stuck . (2) lrnr &#50724;&#48652;&#51229;&#53944; &#49373;&#49457; . lrnr = vision_learner(dls, resnet34, metrics = error_rate) . (3) lrnr &#54617;&#49845; . lrnr.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.150615 | 0.036099 | 0.011502 | 00:35 | . epoch train_loss valid_loss error_rate time . 0 | 0.063639 | 0.034654 | 0.009472 | 00:38 | . (4) lrnr &#50696;&#52769; . lrnr.predict(path.ls()[0]) # 방법1 # lrnr.predict(&#39;2022-09-06-hani03.jpg&#39;) 방법2 # X, y = dls.one_batch() # lrnr.model(X[0 : 1]) 방법3 . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 9.7220e-09])) . &#54532;&#47196;&#44536;&#47000;&#48141; &#44284;&#51221; overview . dls &#50724;&#48652;&#51229;&#53944; &#49373;&#49457; --&gt; lrnr &#50724;&#48652;&#51229;&#53944; &#49373;&#49457; --&gt; lrnr &#54617;&#49845; --&gt; lrnr &#50696;&#52769; . &#48708;&#44368; . 회귀분석(R) 이미지분석(CNN) 추천시스템 . 1단계 | data.frame() | ImageDataLoaders.from_name_func() | CollabDataLoaders.from_df() | . 2단계 | None | cnn_learner() | collab_learner() | . 3단계 | lm(y~x1+x2,df) | lrnr.fine_tune(1) | lrnr.fit() | . 4단계 | predict(ob,newdf) | lrnr.predict(), lrnr.model(X) | lrnr.model(X) | . ImageDataLoaders.from_name_func? # 함수의 경로 확인 가능 . cnn_learner? . vision_learner? . lrnr.fine_tune? . lrnr.predict? . - IAB 딥러닝 수업 자료를 기반으로 공부한 내용입니다. .",
            "url": "https://semibro.github.io/socket/2022/09/08/IABDL.html",
            "relUrl": "/2022/09/08/IABDL.html",
            "date": " • Sep 8, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "IAB딥러닝 9월 6일",
            "content": "Import . from fastai.vision.all import * . &#45936;&#51060;&#53552; &#51200;&#51109; . path = untar_data(URLs.PETS) / &#39;images&#39; . path . Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images&#39;) . PILImage.create(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39;) . files = get_image_files(path) files . (#7390) [Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39;),Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_10.jpg&#39;),Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_100.jpg&#39;),Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_101.jpg&#39;),Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_102.jpg&#39;),Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_103.jpg&#39;),Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_104.jpg&#39;),Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_105.jpg&#39;),Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_106.jpg&#39;),Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_107.jpg&#39;)...] . files[0] . Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39;) . PILImage.create(files[0]) . print(files[2]) PILImage.create(files[2]) . C: Users USER .fastai data oxford-iiit-pet images Abyssinian_100.jpg . print(files[3]) PILImage.create(files[3]) . C: Users USER .fastai data oxford-iiit-pet images Abyssinian_101.jpg . print(files[4]) PILImage.create(files[4]) . C: Users USER .fastai data oxford-iiit-pet images Abyssinian_102.jpg . print(files[5]) PILImage.create(files[5]) . C: Users USER .fastai data oxford-iiit-pet images Abyssinian_103.jpg . print(files[6]) PILImage.create(files[6]) . C: Users USER .fastai data oxford-iiit-pet images Abyssinian_104.jpg . print(files[7]) PILImage.create(files[7]) . C: Users USER .fastai data oxford-iiit-pet images Abyssinian_105.jpg . print(files[8]) PILImage.create(files[8]) . C: Users USER .fastai data oxford-iiit-pet images Abyssinian_106.jpg . def label_func(fname): if fname[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms = Resize(224)) . Due to IPython and Windows limitation, python multiprocessing isn&#39;t available now. So `number_workers` is changed to 0 to avoid getting stuck . dls.show_batch(max_n=16) . &#54617;&#49845; . clsfr = cnn_learner(dls, resnet34, metrics = error_rate) . clsfr.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.150200 | 0.021279 | 0.006766 | 00:35 | . epoch train_loss valid_loss error_rate time . 0 | 0.059970 | 0.032037 | 0.010149 | 00:39 | . &#44592;&#51316; &#45936;&#51060;&#53552;&#47484; &#53685;&#54644; &#51221;&#54869;&#46020; &#52769;&#51221; . files[0] # 고양이 . Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg&#39;) . clsfr.predict(files[0]) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 3.2232e-09])) . files[7] # 고양이 . Path(&#39;C:/Users/USER/.fastai/data/oxford-iiit-pet/images/Abyssinian_105.jpg&#39;) . clsfr.predict(files[7]) . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 4.8954e-11])) . clsfr.show_results() . &#50724;&#45813;&#48516;&#49437; . interpreter = Interpretation.from_learner(clsfr) . interpreter.plot_top_losses(16) . - IAB 딥러닝 수업 자료를 기반으로 공부한 내용입니다. .",
            "url": "https://semibro.github.io/socket/2022/09/06/IABDL.html",
            "relUrl": "/2022/09/06/IABDL.html",
            "date": " • Sep 6, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About Me! . 데이터 분석과 인공지능 개발에 관심이 있습니다. | . Contact . Mail | wnsgud6232@naver.com | . | github | https://github.com/Semibro | . | . 경력 . 전북대학교 바이오메디컬공학부(헬스케어정보전공) (2017-03-01 ~ 2023-02-28) . | 전북대학교 IAB융합전공(IoT, AI, Big Data) (2021-03-01 ~ 2023-02-28) . | 전북대학교 의광학연구실(Vision AI) (2022-06-01 ~ ) . | . 수상 . 전북대학교 캡스톤디자인 경진대회 은상(전기/전자/IT) (2022-06-27) | . 프로젝트 . 한이음ICT멘토링 (2022-04-12 ~ 2022-11-30) | . 자격증 . 자동차운전면허 1종보통 (2017-01-25) . | 한국사능력검정시험 2급 (2019-11-08) . | 컴퓨터활용능력 1급 (2021-07-16) . | 정보처리기사 (2022-09-02) . | . 논문 . IoT센서를 활용한 환경을 생각하는 푸드쉐어링 시스템 구현 및 고찰(Sharing food system implementation that considers the environment using IoT sensors) | . 프로그래밍 언어 . Python . | R . | . 관심분야 . 인공지능 (머신러닝, 딥러닝) . | 빅데이터 . | 데이터 분석 . | .",
          "url": "https://semibro.github.io/socket/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://semibro.github.io/socket/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}